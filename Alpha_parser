from random import choice
import os
import requests
from time import sleep
import time
from urllib.parse import urlparse
from http_request_randomizer.requests.proxy.requestProxy import RequestProxy
from fake_useragent import UserAgent


def proxies():
    if os.path.isfile('proxies.txt'):
        pass
    else:
        req_proxy = RequestProxy()
        PROXIES = "{0}".format(list(map(lambda x: x.get_address(), req_proxy.get_proxy_list())))
        # Open the file for writing
        F = open('proxies.txt', 'w')
        F.writelines(PROXIES)
        F.close()
        pass

def loadRSS():

    file1 = open("proxies.txt", "r")
    PROXIES2 = file1.readlines()


    url1 = {'BTC/USD':'https://btc-alpha.com/api/v1/orderbook/BTC_USD'}
    url2 = {'LTC/USD':'https://btc-alpha.com/api/v1/orderbook/LTC_USD'}
    url3 = {'ETH/USD':'https://btc-alpha.com/api/v1/orderbook/ETH_USD'}
    url4 = {'XRP/USD':'https://btc-alpha.com/api/v1/orderbook/XRP_USD'}
    url5 = {'USD/USDT':'https://btc-alpha.com/api/v1/orderbook/USD_USDT'}
    url6 = {'BTC/USDT':'https://btc-alpha.com/api/v1/orderbook/BTC_USDT'}
    url7 = {'ETH/USDT':'https://btc-alpha.com/api/v1/orderbook/ETH_USDT'}
    url8 = {'XRP/BTC':'https://btc-alpha.com/api/v1/orderbook/XRP_BTC'}
    url9 = {'ETH/BTC':'https://btc-alpha.com/api/v1/orderbook/ETH_BTC'}
    url10 = {'LTC/BTC':'https://btc-alpha.com/api/v1/orderbook/LTC_BTC'}
    url11 = {'BCH/BTC':'https://btc-alpha.com/api/v1/orderbook/BCH_BTC'}
    url12 = {'ZEC/BTC':'https://btc-alpha.com/api/v1/orderbook/ZEC_BTC'}


    urls = [url1, url2, url3,
            url4, url5, url6,url7,url8,url9,url10,url11,url12]
    alpha = {}

    def set_proxy(session, proxy_candidates=PROXIES2, verify=False):
        """
        Configure the session to use one of the proxy_candidates.  If verify is
        True, then the proxy will have been verified to work.
        """
        while True:
            proxy = choice(proxy_candidates)
            session.proxies = {urlparse(proxy).scheme: proxy}
            if not verify:
                return
            try:
                print(session.get('https://httpbin.org/ip').json())
                return
            except Exception:
                session.proxies = {urlparse(next(proxy)).scheme: proxy}
                print("EXCEPTION")
                pass

    def scrape_page(k, url):
        ua = UserAgent()
        session = requests.Session()
        session.headers = {'User-Agent': ua.random}
        set_proxy(session)


        while True:
            try:
                resp = session.get(url)
                v = resp.json()
                # data.update({k: resp.json()})
                alpha.update({k: {'sell': [float(v['sell'][0]['price']), float(v['sell'][0]['amount'])], 'buy':[float(v['buy'][0]['price']), float(v['buy'][0]['amount'])]}})
                break
            except Exception as e:
                session.headers = {'User-Agent': ua.random}
                set_proxy(session, verify=True)
                sleep(0.1)

    for i in urls:
        for k, item in i.items():
            scrape_page(k, item)


    return alpha


if __name__ == "__main__":
    start = time.process_time()
    start11 = time.process_time()
    proxies()
    print('Proxies TIME :', (time.process_time() - start11))
    print(loadRSS())
    print('ALPHA start TIME : ',(time.process_time() - start))

